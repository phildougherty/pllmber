#!/usr/bin/env python3
import argparse
import subprocess
import sys
import os

def main():
    parser = argparse.ArgumentParser(description="Process data through an LLM.")
    parser.add_argument('-m', '--model', help="Name of the LLM model to use. If not specified, the default model is used.")
    parser.add_argument('-p', '--prompt', help="Prompt to send to the LLM.")
    parser.add_argument('shorthand_prompt', nargs='?', help="Shorthand method for entering the prompt by putting it in quotes.")

    args = parser.parse_args()

    default_model = os.getenv('PLLMBER_DEFAULT_MODEL', 'llama3.1:8b')
    model = args.model if args.model else default_model
    prompt = args.prompt if args.prompt else args.shorthand_prompt

    if not prompt:
        print("Error: No prompt provided. Use -p option or provide a shorthand prompt in quotes.")
        sys.exit(1)

    data = sys.stdin.read()
    full_input = f"### Role \nYou are an AI agent playing the role of a unix command line utility. Your job is to help process and make sense of command output and log data. Do not discuss anything, your job is to execute and produce results. Your output should be the result ofthe result of  taking the input data, processing it as described by the user, and returning the resulting output. Your output is short, concise, too the point, and matches easily with other unix command line utilities for further processing unless otherwise directed.{prompt}\n\n{data}"

    try:
        result = subprocess.run(
            ["ollama", "run", model, full_input],
            text=True,
            capture_output=True
        )

        if result.returncode != 0:
            print(f"Error: {result.stderr}")
            sys.exit(result.returncode)

        print(result.stdout)
    except FileNotFoundError as e:
        print(f"Error: {e}")
        print("Make sure 'ollama' is correctly installed and available in your PATH.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    main()
